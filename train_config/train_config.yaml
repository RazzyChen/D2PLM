# DIT (Diffusion Transformer) 训练配置 - v2 (Wandb, Fused AdamW, Custom LR Schedule)

defaults:
  - _self_

# 1. 模型配置 (DITConfig)
model:
  tokenizer: "facebook/esm2_t33_650M_UR50D"
  vocab_size: 33
  max_position_embeddings: 1024
  hidden_size: 1024
  num_hidden_layers: 24
  num_attention_heads: 16
  intermediate_size: 4096
  time_embedding_dim: 256
  hidden_dropout_prob: 0
  attention_probs_dropout_prob: 0
  layer_norm_eps: 1e-5
  initializer_range: 0.02

# 2. 扩散调度器配置 (DITDiffusionScheduler)
diffusion:
  num_train_timesteps: 1000
  beta_start: 1e-4
  beta_end: 0.02
  beta_schedule: "linear"
  prediction_type: "epsilon"
  steps_offset: 0

# 3. 数据配置 (DataPipe)
data:
  train_lmdb_path: "/workspace/d2plm/prepared_dataset/train_lmdb"
  val_lmdb_path: "/workspace/d2plm/prepared_dataset/validation_lmdb"
  train_cache_dir: "/workspace/d2plm/data_cache/train"
  val_cache_dir: "/workspace/d2plm/data_cache/validation"
  max_length: 1024
  batch_size: 8

# 4. 训练核心配置 (TrainingArguments)
training:
  max_steps: 150000
  learning_rate: 4e-4
  weight_decay: 0.01
  max_grad_norm: 0.0
  gradient_accumulation_steps: 32
  
  # 优化器与学习率调度器
  optim: "adamw_torch_fused"
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1

  # 保存、日志和评估频率
  save_steps: 500
  logging_steps: 25
  evaluation_strategy: "steps"
  eval_steps: 500
  
  # 检查点管理
  save_total_limit: 20
  load_best_model_at_end: false 
  deepspeed: "/workspace/d2plm/train_config/ZERO2.json"
  ckpt: null

# 5. 系统与环境配置
system:
  device: "cuda"
  seed: 42
  mixed_precision: "fp16"
  dataloader_num_workers: 11

# 6. Ray - 分布式训练配置
ray:
  run_name_prefix: "D2PLM"
  num_workers: 2
  use_gpu: true
  resources_per_worker:
    CPU: 23
    GPU: 1

# 7. 输出与日志配置
output:
  output_dir: "/workspace/d2plm/outputs/dit_model"

logging:
  report_to: "wandb" # 使用wandb记录日志
  logging_strategy: "steps"

# WandB specific configuration
wabdb:
  project: "D2PLM_DiT"
  entity: null # Replace with your wandb entity if needed

# 8. 策略配置

checkpointing:
  save_strategy: "steps"