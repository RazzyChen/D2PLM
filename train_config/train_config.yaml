# DIT (Diffusion Transformer) 训练配置 - v2 (Wandb, Fused AdamW, Custom LR Schedule)

defaults:
  - _self_

# 1. 模型配置 (DITConfig)
model:
  tokenizer: "facebook/esm2_t33_650M_UR50D"
  vocab_size: 25
  max_position_embeddings: 512
  hidden_size: 1024
  num_hidden_layers: 24
  num_attention_heads: 16
  intermediate_size: 4096
  time_embedding_dim: 256
  hidden_dropout_prob: 0
  attention_probs_dropout_prob: 0
  layer_norm_eps: 1e-12
  initializer_range: 0.02

# 2. 扩散调度器配置 (DITDiffusionScheduler)
diffusion:
  num_train_timesteps: 1000
  beta_start: 1e-4
  beta_end: 0.02
  beta_schedule: "linear"
  prediction_type: "epsilon"
  steps_offset: 0

# 3. 数据配置 (DataPipe)
data:
  lmdb_path: "/workspace/d2plm/dataset"
  max_length: 512
  batch_size: 16
  cache_dir: "/workspace/d2plm/data_cache"

# 4. 训练核心配置 (TrainingArguments)
training:
  num_epochs: 10
  learning_rate: 1e-4
  weight_decay: 0.01
  max_grad_norm: 1.0
  gradient_accumulation_steps: 4
  
  # 优化器与学习率调度器
  optim: "adamw_torch_fused" # 使用fused AdamW
  lr_scheduler_type: "cosine" # 使用cosine衰减
  warmup_ratio: 0.1 # 10%的训练步数用于warmup

  # 保存、日志和评估频率
  save_steps: 5000
  logging_steps: 100
  eval_steps: 1000
  
  # 检查点管理
  save_total_limit: 3
  load_best_model_at_end: false # 放弃在训练结束时加载最佳模型
  deepspeed: "/workspace/d2plm/train_config/ZERO2.yaml" # DeepSpeed配置文件路径
  ckpt: null # 手动指定检查点路径以恢复训练, e.g., "outputs/dit_model/checkpoint-5000"

# 5. 系统与环境配置
system:
  device: "cuda"
  seed: 42
  mixed_precision: "fp16"
  dataloader_num_workers: 4

# 6. Ray - 分布式训练配置
ray:
  run_name_prefix: "D2PLM"
  num_workers: 2
  use_gpu: true
  resources_per_worker:
    CPU: 11
    GPU: 1

# 7. 输出与日志配置
output:
  output_dir: "/workspace/d2plm/outputs/dit_model"

logging:
  report_to: "wandb" # 使用wandb记录日志
  logging_strategy: "steps"

# WandB specific configuration
wandb:
  project: "D2PLM_DiT"
  entity: null # Replace with your wandb entity if needed

# 8. 策略配置

checkpointing:
  save_strategy: "steps"
