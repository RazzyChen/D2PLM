# DIT模型训练配置 (针对 2080 GPU & 12-core CPU)

# 模型相关配置
model:
  pretrained_model_name: "bert-base-chinese" # 预训练分词器路径
  max_position_embeddings: 512
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  time_embedding_dim: 512
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  layer_norm_eps: 1e-12
  initializer_range: 0.02

# 扩散模型相关配置
diffusion:
  num_train_timesteps: 1000
  beta_start: 0.0001
  beta_end: 0.02
  beta_schedule: "linear"
  prediction_type: "epsilon" # 或 "sample"
  steps_offset: 1

# 数据相关配置
data:
  train_path: "/path/to/your/train_data.txt" # !!! 请修改为您的训练数据路径 !!!
  cache_dir: "./cache"
  max_length: 512
  batch_size: 64 # 数据预处理时的批次大小
  preprocessing_num_proc: 10 # 使用10个CPU核心进行数据预处理

# 系统相关配置
system:
  seed: 42
  device: "cuda" # 使用GPU
  compile_model: true # 为支持的GPU架构（如Ampere）启用torch.compile以加速

# 训练相关配置 (Hugging Face Trainer)
training:
  output_dir: "./dit_output"
  num_train_epochs: 5
  per_device_train_batch_size: 16 # 2080显卡建议值, 如OOM请降低
  gradient_accumulation_steps: 2
  learning_rate: 5e-5
  weight_decay: 0.01
  warmup_steps: 500
  logging_steps: 50
  save_steps: 1000
  eval_steps: 1000
  save_total_limit: 3
  dataloader_num_workers: 10 # 使用10个CPU核心加载数据
  remove_unused_columns: true
  push_to_hub: false
  report_to: "tensorboard" # 或 "wandb"

  # 优化与性能
  optim: "adamw_torch"
  bf16: false # 2080 (Turing) 不完全支持bf16, 建议使用fp16
  # fp16: true # 如果需要混合精度训练，可以开启此项
  gradient_checkpointing: true # 节省显存
  max_grad_norm: 1.0

  # 评估与保存
  evaluation_strategy: "steps"
  metric_for_best_model: "loss"
  greater_is_better: false
  load_best_model_at_end: true
  save_strategy: "steps"
  save_safetensors: true
